


# Import necessary libraries
import pandas as pd  # for data handling
import numpy as np  # for mathematical operations
import matplotlib.pyplot as plt  # for data visualization
import seaborn as sns  # for advanced data visualization
from sklearn.model_selection import train_test_split  # for splitting data into training and test sets
from sklearn.preprocessing import LabelEncoder  # for encoding categorical data
from sklearn.ensemble import RandomForestClassifier  # machine learning model
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix  # for model evaluation





df = pd.read_csv("mushrooms.csv")


#data.head() shows the first few rows, giving us a glimpse of the data structure and values.

df.head()








# Basic information about the dataset
df.info()
df.describe(include='all')  # includes statistics for categorical data





















import seaborn as sns
import matplotlib.pyplot as plt

# Plot the distribution of the target variable
sns.countplot(x='class', data=df)
plt.title("Distribution of Edible vs Poisonous Mushrooms")
plt.show()









import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from imblearn.over_sampling import SMOTE

# Load your dataset
df = pd.read_csv('mushrooms.csv')

# Check for class imbalance
class_counts = df['class'].value_counts()
print("Class distribution before handling imbalance:\n", class_counts)

# Separate features and target
X = df.drop(columns='class')
y = df['class']

# One-hot encode categorical variables
X_encoded = pd.get_dummies(X, drop_first=True)

# Ensure all data is numeric by converting any boolean columns to integers
X_encoded = X_encoded.applymap(lambda x: int(x) if isinstance(x, bool) else x)

# Apply SMOTE (oversampling) to balance the classes
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_encoded, y)

# Confirm the new class distribution
new_class_counts = pd.Series(y_resampled).value_counts()
print("Class distribution after SMOTE:\n", new_class_counts)

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Initialize the model
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, pos_label='p')
recall = recall_score(y_test, y_pred, pos_label='p')
f1 = f1_score(y_test, y_pred, pos_label='p')

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")












from sklearn.preprocessing import LabelEncoder

# Encode each column with LabelEncoder
encoder = LabelEncoder()
for column in df.columns:
    df[column] = encoder.fit_transform(df[column])

# Verify encoding
df.head()






from sklearn.model_selection import train_test_split

# Separate features and target variable
X = df.drop('class', axis=1)  # Features
y = df['class']  # Target

# Split the dataset into training (70%) and testing (30%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)









from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score



# Initialize models
models = {
    'Random Forest': RandomForestClassifier(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'SVM': SVC(),
    'KNN': KNeighborsClassifier(),
    'Naive Bayes': GaussianNB()
}

# Train each model and evaluate accuracy
for name, model in models.items():
    model.fit(X_train, y_train)  # Train model
    y_pred = model.predict(X_test)  # Make predictions on test data
    accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy
    print(f"{name} Accuracy: {accuracy:.2f}")













